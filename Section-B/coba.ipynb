{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class FeedForwardNN:\n",
    "    def __init__(self, input_size, hidden_layers, activations, output_size, learning_rate=0.001, max_iter=1000,\n",
    "                 tol=1e-4, batch_size=32, random_state=None):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.activations = activations\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.batch_size = batch_size\n",
    "        if random_state:\n",
    "            np.random.seed(random_state)\n",
    "\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.activation_functions = []\n",
    "        self.activation_derivatives = []\n",
    "\n",
    "        self._initialize_weights_and_biases()\n",
    "        self._initialize_activation_functions()\n",
    "\n",
    "    def _initialize_weights_and_biases(self):\n",
    "        layer_sizes = [self.input_size] + self.hidden_layers + [self.output_size]\n",
    "        for i in range(1, len(layer_sizes)):\n",
    "            # Initialize weights with random values from a normal distribution\n",
    "            weight_matrix = np.random.randn(layer_sizes[i - 1], layer_sizes[i])\n",
    "            # Initialize biases with random values from a normal distribution\n",
    "            bias_vector = np.random.randn(layer_sizes[i])\n",
    "            self.weights.append(weight_matrix)\n",
    "            self.biases.append(bias_vector)\n",
    "\n",
    "    def _initialize_activation_functions(self):\n",
    "        for activation in self.activations:\n",
    "            if activation == 'relu':\n",
    "                self.activation_functions.append(self._relu)\n",
    "                self.activation_derivatives.append(self._relu_derivative)\n",
    "            elif activation == 'sigmoid':\n",
    "                self.activation_functions.append(self._sigmoid)\n",
    "                self.activation_derivatives.append(self._sigmoid_derivative)\n",
    "            elif activation == 'linear':\n",
    "                self.activation_functions.append(self._linear)\n",
    "                self.activation_derivatives.append(self._linear_derivative)\n",
    "            elif activation == 'softmax':\n",
    "                self.activation_functions.append(self._softmax)\n",
    "                self.activation_derivatives.append(self._softmax_derivative)\n",
    "\n",
    "    @staticmethod\n",
    "    def _relu(x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    @staticmethod\n",
    "    def _linear(x):\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def _softmax(x):\n",
    "        exp_scores = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "    def _relu_derivative(self, x):\n",
    "        x[x <= 0] = 0\n",
    "        x[x > 0] = 1\n",
    "        return x\n",
    "\n",
    "    def _sigmoid_derivative(self, x):\n",
    "        return self._sigmoid(x) * (1 - self._sigmoid(x))\n",
    "\n",
    "    def _linear_derivative(self, x):\n",
    "        return np.ones_like(x)\n",
    "\n",
    "    def _softmax_derivative(self, x):\n",
    "        s = self._softmax(x)\n",
    "        return s * (1 - s)\n",
    "\n",
    "    def _feedforward(self, X):\n",
    "        activations = [X]\n",
    "        for i in range(len(self.weights)):\n",
    "            z = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
    "            activations.append(self.activation_functions[i](z))\n",
    "        return activations\n",
    "\n",
    "    def _backpropagation(self, X_batch, y_batch, activations_batch):\n",
    "        m = X_batch.shape[0]\n",
    "        gradient_weights = [np.zeros_like(w) for w in self.weights]\n",
    "        gradient_biases = [np.zeros_like(b) for b in self.biases]\n",
    "\n",
    "        # Compute error in output layer\n",
    "        error = activations_batch[-1] - y_batch\n",
    "\n",
    "        # Backpropagate the error\n",
    "        for i in range(len(self.weights) - 1, -1, -1):\n",
    "            gradient_biases[i] = np.sum(error, axis=0) / m\n",
    "            gradient_weights[i] = np.dot(activations_batch[i].T, error) / m\n",
    "            if i > 0:\n",
    "                error = np.dot(error, self.weights[i].T) * self.activation_derivatives[i](activations_batch[i])\n",
    "\n",
    "        return gradient_weights, gradient_biases\n",
    "\n",
    "    def _update_weights_and_biases(self, gradient_weights, gradient_biases):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] = self.weights[i] + self.learning_rate * -(gradient_weights[i])\n",
    "            self.biases[i] = self.biases[i] + self.learning_rate * -(gradient_biases[i])\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        one_hot_encoder = OneHotEncoder()\n",
    "        y_encoded = one_hot_encoder.fit_transform(y.reshape(-1, 1)).toarray()\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            # Shuffle data\n",
    "            indices = np.arange(X.shape[0])\n",
    "            np.random.shuffle(indices)\n",
    "            X_shuffled = X.iloc[indices]\n",
    "            y_shuffled = y_encoded[indices]\n",
    "\n",
    "            # Split data into batches\n",
    "            for batch_start in range(0, X.shape[0], self.batch_size):\n",
    "                X_batch = X_shuffled.iloc[batch_start:batch_start+self.batch_size]\n",
    "                y_batch = y_shuffled[batch_start:batch_start+self.batch_size]\n",
    "\n",
    "                activations_batch = self._feedforward(X_batch.values)\n",
    "                gradient_weights, gradient_biases = self._backpropagation(X_batch.values, y_batch, activations_batch)\n",
    "                self._update_weights_and_biases(gradient_weights, gradient_biases)\n",
    "\n",
    "            # Calculate loss (MSE) for the entire dataset\n",
    "            activations = self._feedforward(X.values)\n",
    "            \n",
    "            if self.activations[-1] != 'softmax':\n",
    "                loss = np.mean(np.square(activations[-1] - y_encoded))\n",
    "            else:\n",
    "                loss = -np.mean(np.sum(y_encoded * np.log(activations[-1]), axis=1))\n",
    "\n",
    "            if loss < self.tol:\n",
    "                break\n",
    "\n",
    "    def predict(self, X):\n",
    "        activations = self._feedforward(X.values)\n",
    "        return np.argmax(activations[-1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.97\n",
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 1 2 2 2 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Load data from CSV\n",
    "data = pd.read_csv('models/iris.csv')\n",
    "\n",
    "# Reset index to ensure alignment after shuffling\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Preprocess data\n",
    "X = data.drop(columns=['Species', 'Id'])\n",
    "y = data['Species']\n",
    "\n",
    "# Encode categorical labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define network architecture\n",
    "input_size = X_train.shape[1]\n",
    "hidden_layers = [10, 5]  # Example: 2 hidden layers with 10 and 5 neurons\n",
    "output_size = len(np.unique(y_train))  # Number of classes in the output layer\n",
    "activations = ['relu', 'relu', 'softmax']  # Example: ReLU activation for hidden layers, Softmax for output layer\n",
    "\n",
    "# Train and evaluate the model\n",
    "model = FeedForwardNN(input_size=input_size, hidden_layers=hidden_layers, activations=activations,\n",
    "                      output_size=output_size, learning_rate=0.001, max_iter=1000, tol=1e-4, batch_size=100,\n",
    "                      random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 5.53833006e-01,  8.82165144e-01,  8.22300431e-01,\n",
      "        -7.06161479e-01],\n",
      "       [-2.02272885e-01, -3.95715375e-01, -8.54365851e-02,\n",
      "        -3.38212766e-01],\n",
      "       [ 4.43767470e-02,  3.16145392e-01, -3.32323997e-01,\n",
      "         1.21725028e+00],\n",
      "       [ 8.34625303e-01, -2.69601867e-01,  5.37637370e-04,\n",
      "        -1.50138873e+00],\n",
      "       [ 2.35781372e-01,  2.27771134e-01,  4.97546745e-01,\n",
      "        -1.31345241e+00]]), array([[-0.83603583,  0.07835199, -0.95442628],\n",
      "       [-0.52551007,  0.03165069, -0.87777477],\n",
      "       [-0.17176614, -0.17532494, -0.54611671],\n",
      "       [ 2.35990022,  2.32848588,  1.53583121]]), array([[ 1.23614441e+00, -1.75417434e-02],\n",
      "       [ 1.09572712e+00, -6.15946830e-02],\n",
      "       [ 2.02640968e+00, -2.17999655e-04]]), array([[ 4.20790952e-01,  3.03805380e-01, -1.97721465e+00],\n",
      "       [ 6.43628311e-02,  1.07635724e-03,  4.50708769e-02]])]\n",
      "Accuracy: 1.00\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00        10\n",
      "Iris-versicolor       1.00      1.00      1.00         9\n",
      " Iris-virginica       1.00      1.00      1.00        11\n",
      "\n",
      "       accuracy                           1.00        30\n",
      "      macro avg       1.00      1.00      1.00        30\n",
      "   weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load data from CSV\n",
    "data = pd.read_csv('models/iris.csv')\n",
    "\n",
    "# Preprocess data\n",
    "X = data.drop(columns=['Species'])\n",
    "y = data['Species']\n",
    "\n",
    "# Encode categorical labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Set hyperparameters\n",
    "hidden_layer_sizes = (4,3,2) # 2 hidden layers, 10 neurons in the first layer, 5 neurons in the second layer\n",
    "activation_functions = 'relu'\n",
    "learning_rate = 0.001\n",
    "error_threshold = 0.0001\n",
    "max_iterations = 1000\n",
    "batch_size = 100\n",
    "\n",
    "# Train model\n",
    "model = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, activation=activation_functions, learning_rate_init=learning_rate, tol=error_threshold, max_iter=max_iterations, batch_size=batch_size, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Print the weights matrices\n",
    "print(model.coefs_)\n",
    " \n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    " \n",
    "# Calculate the accuracy of the model\n",
    "accuracy = (y_pred == y_test).mean()\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Evaluate model\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
